# I2VGen-XL

[I2VGen-XL: High-Quality Image-to-Video Synthesis via Cascaded Diffusion Models](https://hf.co/papers/2311.04145.pdf) بقلم شي وي تشانغ، وجيا يو وانغ، ويينجيا تشانغ، وكانج تشاو، وهانججي يوان، وزي وو تشين، وشيانيانج وانج، وديلي تشاو، وجينجرين تشو.

ملخص الورقة هو:

*حققت توليف الفيديو مؤخرًا تقدمًا ملحوظًا بفضل التطور السريع لنماذج الانتشار. ومع ذلك، لا يزال يواجه تحديات من حيث الدقة الدلالية والوضوح والاستمرارية المكانية-الزمانية. وينشأ ذلك في المقام الأول عن ندرة البيانات النصية-الفيديو المحاذاة جيدًا والهيكل المتأصل المعقد للفيديوهات، مما يجعل من الصعب على النموذج أن يضمن في نفس الوقت التفوق الدلالي والنوعي. في هذا التقرير، نقترح نهجًا متتاليًا لـ I2VGen-XL يحسن أداء النموذج من خلال فصل هذين العاملين ويضمن محاذاة بيانات الإدخال باستخدام الصور الثابتة كشكل من أشكال التوجيه الأساسي. يتكون I2VGen-XL من مرحلتين: 'i) تضمن مرحلة الأساس تماسك الدلالات وتحافظ على المحتوى من الصور المدخلة باستخدام مشفرين هرميين، و'ii) مرحلة التحسين التي تعزز تفاصيل الفيديو من خلال دمج نص موجز إضافي وتحسين الدقة إلى 1280x720. لزيادة التنوع، نقوم بجمع حوالي 35 مليون زوج من النصوص والفيديوهات ذات اللقطة الواحدة و6 مليارات زوج من النصوص والصور لتحسين النموذج. بهذه الطريقة، يمكن لـ I2VGen-XL أن تعزز في نفس الوقت الدقة الدلالية واستمرارية التفاصيل ووضوح الفيديوهات المولدة. من خلال التجارب المستفيضة، قمنا بدراسة المبادئ الأساسية لـ I2VGen-XL وقارناها بالطرق الحالية الرائدة، والتي يمكن أن توضح فعاليتها على البيانات المتنوعة. ستكون الشفرة المصدرية والنماذج متاحة للجمهور على [هذا الرابط https](https://i2vgen-xl.github.io/).*

يمكن العثور على الشفرة الأساسية الأصلية [هنا](https://github.com/ali-vilab/i2vgen-xl/). ويمكن العثور على نقاط تفتيش النموذج [هنا](https://huggingface.co/ali-vilab/).

<Tip>
تأكد من الاطلاع على دليل Schedulers [../../using-diffusers/schedulers] لمعرفة كيفية استكشاف التوازن بين سرعة المجدول والجودة، وقسم "إعادة استخدام المكونات عبر الأنابيب" [../../using-diffusers/loading#reuse-components-across-pipelines] لمعرفة كيفية تحميل المكونات نفسها بكفاءة في أنابيب متعددة. أيضًا، لمعرفة المزيد حول تقليل استخدام الذاكرة لهذا الأنبوب، راجع قسم "تقليل استخدام الذاكرة" [هنا](../../using-diffusers/svd#reduce-memory-usage).
</Tip>

الناتج النموذجي مع I2VGenXL:

|  |  |
| --- | --- |
| <center>المكتبة.<br><img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/i2vgen-xl-example.gif" alt="library" style="width: 300px;" /></center> |  |

## ملاحظات

* يستخدم I2VGenXL دائمًا قيمة `clip_skip` تساوي 1. وهذا يعني أنه يستفيد من تمثيلات الطبقة قبل الأخيرة من مشفر النص CLIP.
* يمكنه توليد فيديوهات بجودة تضاهي في كثير من الأحيان [Stable Video Diffusion](../../using-diffusers/svd) (SVD).
* على عكس SVD، فإنه يقبل أيضًا مطالبات نصية كمدخلات.
* يمكنه توليد فيديوهات بدقة أعلى.
* عند استخدام [`DDIMScheduler`] (الافتراضي لهذا الأنبوب)، يؤدي استخدام أقل من 50 خطوة للاستنتاج إلى نتائج سيئة.
* هذا التنفيذ هو متغير أحادي المرحلة من I2VGenXL. يظهر الشكل الرئيسي في ورقة [I2VGen-XL](https://arxiv.org/abs/2311.04145) متغيرًا ثنائي المرحلة، ومع ذلك، يعمل المتغير أحادي المرحلة بشكل جيد. راجع [هذا النقاش](https://github.com/huggingface/diffusers/discussions/7952) لمزيد من التفاصيل.

## I2VGenXLPipeline

[[autodoc]] I2VGenXLPipeline

- all
- __call__

## I2VGenXLPipelineOutput

[[autodoc]] pipelines.i2vgen_xl.pipeline_i2vgen_xl.I2VGenXLPipelineOutput