# MusicLDM

تم اقتراح MusicLDM في [MusicLDM: Enhancing Novelty in Text-to-Music Generation Using Beat-Synchronous Mixup Strategies](https://huggingface.co/papers/2308.01546) بواسطة Ke Chen, Yusong Wu, Haohe Liu, Marianna Nezhurina, Taylor Berg-Kirkpatrick, Shlomo Dubnov.

يأخذ MusicLDM كلمات نصية كمدخلات ويتوقع العينة الموسيقية المناسبة. مستوحى من [Stable Diffusion](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/overview) و [AudioLDM](https://huggingface.co/docs/diffusers/api/pipelines/audioldm)، MusicLDM هو نموذج _latent diffusion model (LDM)_ لتوليد الموسيقى من النص، والذي يتعلم التمثيلات الصوتية المستمرة من [CLAP](https://huggingface.co/docs/transformers/main/model_doc/clap) latents.

تم تدريب MusicLDM على مجموعة بيانات موسيقية لمدة 466 ساعة. وتم تطبيق استراتيجيات زيادة البيانات المتزامنة مع الإيقاع على العينات الموسيقية، في كل من النطاق الزمني ومساحة latent. ومن خلال استخدام استراتيجيات زيادة البيانات المتزامنة مع الإيقاع، يتم تشجيع النموذج على الاستيفاء بين عينات التدريب، ولكن مع البقاء ضمن نطاق بيانات التدريب. والنتيجة هي موسيقى مُولدة أكثر تنوعًا مع الحفاظ على أسلوبها المُقابل.

ملخص الورقة هو كما يلي:

* أظهرت نماذج الانتشار نتائج واعدة في مهام التوليد متعددة الوسائط، بما في ذلك توليد النص إلى الصورة والصوت. ومع ذلك، فإن توليد الموسيقى، كنوع خاص من الصوت، يطرح تحديات فريدة بسبب محدودية توفر البيانات الموسيقية والقضايا الحساسة المتعلقة بحقوق النشر والانتحال. في هذه الورقة، ولمواجهة هذه التحديات، نقوم أولاً ببناء نموذج text-to-music متطور، MusicLDM، والذي يتكيف مع Stable Diffusion ومعماريات AudioLDM مع مجال الموسيقى. ونحقق ذلك من خلال إعادة تدريب نموذج pretraining التناقضي للغة الصوت (CLAP) ومشفر Hifi-GAN، كمكونات من MusicLDM، على مجموعة من عينات البيانات الموسيقية. ثم، لمعالجة قيود بيانات التدريب وتجنب الانتحال، نستفيد من نموذج تتبع الإيقاع ونقترح استراتيجيتين مختلفتين للخلط: خلط الصوت المتزامن مع الإيقاع وخلط latent المتزامن مع الإيقاع، والذي يعيد دمج التدريب الصوتي مباشرة أو عبر مساحة latent embeddings، على التوالي. تشجع مثل هذه الاستراتيجيات الخلطية النموذج على الاستيفاء بين العينات الموسيقية التدريبية وتوليد موسيقى جديدة ضمن الغلاف المحدب لبيانات التدريب، مما يجعل الموسيقى المولدة أكثر تنوعًا مع الحفاظ على أسلوبها المُقابل. بالإضافة إلى مقاييس التقييم الشائعة، نقوم بتصميم العديد من مقاييس التقييم الجديدة المستندة إلى CLAP score لإثبات أن MusicLDM المقترح واستراتيجيات الخلط المتزامنة مع الإيقاع تحسن من جودة الموسيقى المولدة وحداثتها، بالإضافة إلى التوافق بين النص المدخل والموسيقى المولدة.

تمت المساهمة بهذه الخطوط العريضة بواسطة [sanchit-gandhi](https://huggingface.co/sanchit-gandhi).

## نصائح

عند بناء موجه، ضع في اعتبارك ما يلي:

* تعمل موجهات الإدخال الوصفية بشكل أفضل؛ استخدم الصفات لوصف الصوت (على سبيل المثال، "عالي الجودة" أو "واضح") وجعل سياق الموجه محددًا قدر الإمكان (على سبيل المثال، يعمل "techno ذو إيقاع سريع وأصوات" بشكل أفضل من "techno").
* يمكن أن يؤدي استخدام موجه سلبي إلى تحسين جودة الصوت المولد بشكل كبير. جرب استخدام موجه سلبي "منخفض الجودة، متوسط الجودة".

أثناء الاستنتاج:

* يمكن التحكم في جودة عينة الصوت المولدة بواسطة حجة `num_inference_steps`؛ حيث توفر الخطوات الأعلى صوتًا بجودة أعلى على حساب الاستدلال البطيء.
* يمكن إنشاء عدة أشكال موجية في مرة واحدة: قم بتعيين `num_waveforms_per_prompt` إلى قيمة أكبر من 1 لتمكينها. سيتم إجراء التصنيف التلقائي بين الموجات الصوتية والنص الموجه، وسيتم تصنيف الأصوات من الأفضل إلى الأسوأ وفقًا لذلك.
* يمكن التحكم في طول عينة الصوت المولدة عن طريق تغيير حجة `audio_length_in_s`.

<Tip>

تأكد من مراجعة دليل Schedulers [guide](../../using-diffusers/schedulers) لمعرفة كيفية استكشاف المقايضة بين سرعة وجودة المجدول، وقسم [إعادة استخدام المكونات عبر الأنابيب](../../using-diffusers/loading#reuse-components-across-pipelines) لمعرفة كيفية تحميل المكونات نفسها بكفاءة في خطوط أنابيب متعددة.

</Tip>

## MusicLDMPipeline

[[autodoc]] MusicLDMPipeline
- all
- __call__